# KNOWLEDGE GRAPH POPULATION

This folder contains scripts used to generate the ABOX (i.e., the instances) of the previously generated Knowledge Graph. It is organized in subfolders, where each of them contains the scripts to generate the triples from a different source:

- **HyperOpt-Sklearn**: In this folder, the scripts to generate Knowledge Graph instances from HyperOpt-Sklearn experiments can be found. With HyperOpt-Sklearn some basic pipelines are created (i.e., they contain up to one *Preprocessing* step plus a *Classification* or *Regression* algorithm), for which the hyperparameters of all the components are captured and the final results are stores. Concretely, it contains two scripts: *hyperopt_experiments.py*, for which the user must provide some datasets as input (assumed to have the target variable) and a pickle file is created, which stores all the important information. Additionally, the script *hyperopt_constraint_experiments.py* generates experiments by randomly forcing some constraints to the optimization algorithm. If the user does not have datasets, it can execute the script *hyperopt_toy.py*, which fetches some of the toy dataset provied by scikit-learn library. Then, the script *hyperopt_results_to_rdf.py* reads the generated pickle files and crerates the RDF instances that will populate the KG.

- **TPOT**: In this folder, the scripts to generate Knowledge Graph instances from TPOT experiments can be found. With TPOT some pipelines (than can be more complex than those generated by HyperOpt-Sklearn) are created, for which the hyperparameters of all the components are captured and the final results are stored. Concretely, it contains two scripts: *tpot_experiments.py*, for which the user must provide some datasets as input (assumed to have the target variable) and a pickle file is created, which stores all the important information. If the user does not have datasets, it can execute the script *tmop_toy.py*, which fetches some of the toy dataset provied by scikit-learn library. Then, the script *tpot_results_to_rdf.py* reads the generated pickle files and crerates the RDF instances that will populate the KG.


Additionally, there are some scripts that can be used to assisit in the creation RDF triples for the Knowledge Graph:

- *meta_features_generator.py*: from a dataset stored in a .csv file, it extracts its features and generates the correponding RDF triples. They can be found inside the *datasets folder*.
- *sklearn_helper.py*: it generates the RDF triples for all the scikit-learn library classification, regression, clustering and preprocessing algorithms. Concretely, it links each algorithm implementation to the general algorithm in DMOP (or it creates it if it does not exist) and it creates the instances for each of the hyperparameters.